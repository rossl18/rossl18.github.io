{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "M-WpOf6LQnDg",
        "outputId": "bc237ffb-0c4f-4615-9ee4-5dd177b63fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: better_profanity in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Collecting pdfminer\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome (from pdfminer)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140060 sha256=9aec7491e137252d4456ee6db2974f6aa2330077845086f0683efdc1c0f52589\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.21.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pdfminer.high_level'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6880d953b666>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_level\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Download necessary NLTK data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer.high_level'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "%pip install better_profanity\n",
        "%pip install pdfminer\n",
        "%pip install nltk\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import vstack\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from better_profanity import profanity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text by lowercasing, removing special characters and numbers,\n",
        "    tokenizing, removing stop words, and applying stemming.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The raw text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "    - preprocessed_text (str): The preprocessed text.\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def process_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file and preprocesses it.\n",
        "\n",
        "    Parameters:\n",
        "    - pdf_path (str): The file path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "    - preprocessed_text (str): The preprocessed text extracted from the PDF.\n",
        "    \"\"\"\n",
        "    raw_text = extract_text(pdf_path)\n",
        "    if not raw_text:\n",
        "        print(f\"No text extracted from {pdf_path}.\")\n",
        "        return \"\"\n",
        "    preprocessed_text = preprocess_text(raw_text)\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def extract_tfidf_features(preprocessed_texts):\n",
        "    \"\"\"\n",
        "    Transforms a list of preprocessed texts into TF-IDF features.\n",
        "\n",
        "    Parameters:\n",
        "    - preprocessed_texts (list of str): A list of preprocessed text documents.\n",
        "\n",
        "    Returns:\n",
        "    - X (scipy.sparse.csr_matrix): The TF-IDF feature matrix.\n",
        "    - vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(preprocessed_texts)\n",
        "    return X, vectorizer\n",
        "\n",
        "\n",
        "def process_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Processes all PDF files in a folder, extracts and preprocesses text,\n",
        "    and extracts TF-IDF features.\n",
        "\n",
        "    Parameters:\n",
        "    - folder_path (str): The path to the folder containing PDF files.\n",
        "\n",
        "    Returns:\n",
        "    - X (scipy.sparse.csr_matrix): The TF-IDF feature matrix for all documents.\n",
        "    - vectorizer (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "    - filenames (list of str): List of filenames corresponding to each document in X.\n",
        "    \"\"\"\n",
        "    preprocessed_texts = []\n",
        "    filenames = []\n",
        "\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(f\"Folder not found: {folder_path}\")\n",
        "        return None, None, []\n",
        "\n",
        "    for filename in sorted(os.listdir(folder_path)):\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            pdf_path = os.path.join(folder_path, filename)\n",
        "            preprocessed_text = process_pdf(pdf_path)\n",
        "            if preprocessed_text:\n",
        "                preprocessed_texts.append(preprocessed_text)\n",
        "                filenames.append(filename)\n",
        "            else:\n",
        "                print(f\"Skipping {filename} due to no extracted text.\")\n",
        "\n",
        "    if not preprocessed_texts:\n",
        "        print(\"No texts were processed from PDFs.\")\n",
        "        return None, None, filenames\n",
        "\n",
        "    X, vectorizer = extract_tfidf_features(preprocessed_texts)\n",
        "    return X, vectorizer, filenames\n",
        "\n",
        "\n",
        "def process_csv_files(csv_file_class0, csv_file_class1, text_column='text'):\n",
        "    \"\"\"\n",
        "    Reads and preprocesses texts from two CSV files for class 0 and class 1.\n",
        "\n",
        "    Parameters:\n",
        "    - csv_file_class0 (str): Path to the CSV file containing texts for class 0.\n",
        "    - csv_file_class1 (str): Path to the CSV file containing texts for class 1.\n",
        "    - text_column (str): Name of the column in the CSV files containing the texts.\n",
        "\n",
        "    Returns:\n",
        "    - preprocessed_texts (list of str): Preprocessed texts from both CSV files.\n",
        "    - labels (list of int): Corresponding labels (0 or 1) for each text.\n",
        "    \"\"\"\n",
        "    preprocessed_texts = []\n",
        "    labels = []\n",
        "\n",
        "    # Process class 0 CSV\n",
        "    df_class0 = pd.read_csv(csv_file_class0)\n",
        "    if text_column not in df_class0.columns:\n",
        "        print(f\"Column '{text_column}' not found in {csv_file_class0}. Available columns: {df_class0.columns.tolist()}\")\n",
        "        return preprocessed_texts, labels\n",
        "    for text in df_class0[text_column].astype(str).tolist():\n",
        "        preprocessed_texts.append(preprocess_text(text))\n",
        "        labels.append(0)\n",
        "\n",
        "    # Process class 1 CSV\n",
        "    df_class1 = pd.read_csv(csv_file_class1)\n",
        "    if text_column not in df_class1.columns:\n",
        "        print(f\"Column '{text_column}' not found in {csv_file_class1}. Available columns: {df_class1.columns.tolist()}\")\n",
        "        return preprocessed_texts, labels\n",
        "    for text in df_class1[text_column].astype(str).tolist():\n",
        "        preprocessed_texts.append(preprocess_text(text))\n",
        "        labels.append(1)\n",
        "\n",
        "    return preprocessed_texts, labels\n",
        "\n",
        "\n",
        "def test_models_and_decision(folder_path, labels, add_text=None, csv_texts=None, csv_labels=None, x_percent=1000):\n",
        "    \"\"\"\n",
        "    Processes PDFs and CSV data, trains and evaluates models, and applies decision rules.\n",
        "\n",
        "    Parameters:\n",
        "    - folder_path (str): Path to the folder containing PDF files.\n",
        "    - labels (list): Labels corresponding to the PDFs and CSV texts.\n",
        "    - add_text (list of str, optional): Additional texts to include.\n",
        "    - csv_texts (list of str, optional): Preprocessed texts from CSV files.\n",
        "    - csv_labels (list of int, optional): Labels corresponding to the CSV texts.\n",
        "    - x_percent (float): Percentage threshold for length comparison.\n",
        "\n",
        "    Returns:\n",
        "    - decisions (list of int): List of binary decisions (1 or 0) for each input.\n",
        "    \"\"\"\n",
        "    # Initialize profanity filter\n",
        "    profanity.load_censor_words()\n",
        "\n",
        "    # Process PDFs\n",
        "    X_pdf, vectorizer, filenames = process_folder(folder_path)\n",
        "    preprocessed_texts_pdf = []\n",
        "    if X_pdf is not None:\n",
        "        for filename in filenames:\n",
        "            pdf_path = os.path.join(folder_path, filename)\n",
        "            text = process_pdf(pdf_path)\n",
        "            preprocessed_texts_pdf.append(text)\n",
        "\n",
        "    # Process additional texts\n",
        "    preprocessed_texts_add = []\n",
        "    if add_text:\n",
        "        preprocessed_texts_add = [preprocess_text(text) for text in add_text]\n",
        "        if vectorizer:\n",
        "            X_add_text = vectorizer.transform(preprocessed_texts_add)\n",
        "        else:\n",
        "            X_add_text = None\n",
        "    else:\n",
        "        X_add_text = None\n",
        "\n",
        "    # Process CSV texts\n",
        "    preprocessed_csv_texts = csv_texts if csv_texts else []\n",
        "    preprocessed_csv_labels = csv_labels if csv_labels else []\n",
        "    if preprocessed_csv_texts and vectorizer:\n",
        "        X_csv_texts = vectorizer.transform(preprocessed_csv_texts)\n",
        "    else:\n",
        "        X_csv_texts = None\n",
        "\n",
        "    # Combine all texts and labels\n",
        "    all_texts = preprocessed_texts_pdf + preprocessed_texts_add + preprocessed_csv_texts\n",
        "    all_labels = labels  # labels already include csv_labels\n",
        "\n",
        "    # Print class distribution\n",
        "    unique, counts = np.unique(all_labels, return_counts=True)\n",
        "    class_distribution = dict(zip(unique, counts))\n",
        "    print(f\"\\nClass Distribution: {class_distribution}\")\n",
        "\n",
        "    # Print total number of texts and labels\n",
        "    print(f\"Total number of texts: {len(all_texts)}\")\n",
        "    print(f\"Total number of labels: {len(all_labels)}\")\n",
        "\n",
        "    # Validate labels and texts count\n",
        "    if len(all_labels) != len(all_texts):\n",
        "        print(f\"Mismatch between number of labels ({len(all_labels)}) and number of texts ({len(all_texts)}).\")\n",
        "        return []\n",
        "\n",
        "    # Combine feature matrices\n",
        "    feature_matrices = []\n",
        "    if X_pdf is not None:\n",
        "        feature_matrices.append(X_pdf)\n",
        "    if X_add_text is not None:\n",
        "        feature_matrices.append(X_add_text)\n",
        "    if X_csv_texts is not None:\n",
        "        feature_matrices.append(X_csv_texts)\n",
        "\n",
        "    if not feature_matrices:\n",
        "        print(\"No data to process.\")\n",
        "        return []\n",
        "\n",
        "    X = vstack(feature_matrices)\n",
        "    y = np.array(all_labels)\n",
        "\n",
        "    # Calculate average text length\n",
        "    lengths = [len(text.split()) for text in all_texts]\n",
        "    avg_length = np.mean(lengths)\n",
        "    print(f\"Average text length: {avg_length:.2f} words\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Initialize models\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
        "        'Support Vector Machine': SVC(probability=True, class_weight='balanced'),\n",
        "        'Naïve Bayes': MultinomialNB()\n",
        "    }\n",
        "\n",
        "    performance_data = []\n",
        "\n",
        "    # Train and evaluate models\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        model_performance = {\n",
        "            'Model': model_name,\n",
        "            'Accuracy': accuracy,\n",
        "            'Macro Precision': report['macro avg']['precision'],\n",
        "            'Macro Recall': report['macro avg']['recall'],\n",
        "            'Macro F1-score': report['macro avg']['f1-score'],\n",
        "            'Weighted Precision': report['weighted avg']['precision'],\n",
        "            'Weighted Recall': report['weighted avg']['recall'],\n",
        "            'Weighted F1-score': report['weighted avg']['f1-score'],\n",
        "            'True Negatives': cm[0][0],\n",
        "            'False Positives': cm[0][1],\n",
        "            'False Negatives': cm[1][0],\n",
        "            'True Positives': cm[1][1]\n",
        "        }\n",
        "\n",
        "        performance_data.append(model_performance)\n",
        "\n",
        "    # Display performance metrics\n",
        "    performance_df = pd.DataFrame(performance_data)\n",
        "    cols = [\n",
        "        'Model', 'Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1-score',\n",
        "        'Weighted Precision', 'Weighted Recall', 'Weighted F1-score',\n",
        "        'True Negatives', 'False Positives', 'False Negatives', 'True Positives'\n",
        "    ]\n",
        "    performance_df = performance_df[cols]\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    print(performance_df)\n",
        "\n",
        "    # Get model predictions on all data\n",
        "    model_predictions = []\n",
        "    for model_name, model in models.items():\n",
        "        preds = model.predict(X)\n",
        "        model_predictions.append(preds)\n",
        "    model_predictions = np.array(model_predictions).T  # Shape: (num_inputs, num_models)\n",
        "\n",
        "    # **Temporary Debugging: Print raw predictions**\n",
        "    print(\"\\nSample Model Predictions:\")\n",
        "    sample_size = min(10, len(all_texts))  # Print first 10 for brevity\n",
        "    for idx in range(sample_size):\n",
        "        preds = model_predictions[idx]\n",
        "        print(f\"Text ID {idx+1}: Predictions={preds}, Label={all_labels[idx]}\")\n",
        "    print(\"...\")  # Indicate more data exists\n",
        "\n",
        "    # Apply decision rules\n",
        "    decisions = []\n",
        "    for idx, text in enumerate(all_texts):\n",
        "        if profanity.contains_profanity(text):\n",
        "            final_decision = 0  # Auto reject\n",
        "        else:\n",
        "            preds = model_predictions[idx]\n",
        "            num_ones = np.sum(preds == 1)\n",
        "            if num_ones >= 2:\n",
        "                length = lengths[idx]\n",
        "                if abs(length - avg_length) <= (x_percent / 100) * avg_length:\n",
        "                    final_decision = 1\n",
        "                else:\n",
        "                    final_decision = 0\n",
        "            else:\n",
        "                final_decision = 0\n",
        "        decisions.append(final_decision)\n",
        "\n",
        "    return decisions\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to process the 'syllabi' folder and test machine learning models with additional CSV data.\n",
        "    \"\"\"\n",
        "    # Set the folder path\n",
        "    folder_path = 'C:/Users/rossl/syllabi'  # Replace with your actual folder path\n",
        "\n",
        "    # Obtain the list of PDF filenames\n",
        "    try:\n",
        "        filenames = sorted([f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Folder not found: {folder_path}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing folder {folder_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Provide labels corresponding to each PDF file\n",
        "    labels_pdf = [0, 0, 0, 0, 0, 1, 1, 1, 1]  # Adjust according to your PDFs\n",
        "\n",
        "    # Check if the number of labels matches the number of files\n",
        "    if len(labels_pdf) != len(filenames):\n",
        "        print(\"The number of labels does not match the number of PDF files.\")\n",
        "        print(f\"Number of labels: {len(labels_pdf)}\")\n",
        "        print(f\"Number of PDF files: {len(filenames)}\")\n",
        "        return\n",
        "\n",
        "    # Paths to the CSV files\n",
        "    csv_file_class0 = 'C:/Users/rossl/nonsyllabi.csv'  # Replace with your actual path\n",
        "    csv_file_class1 = 'C:/Users/rossl/syllabi.csv'     # Replace with your actual path\n",
        "\n",
        "    # Ensure the CSV files exist\n",
        "    if not os.path.exists(csv_file_class0):\n",
        "        print(f\"CSV file for class 0 does not exist: {csv_file_class0}\")\n",
        "        return\n",
        "    if not os.path.exists(csv_file_class1):\n",
        "        print(f\"CSV file for class 1 does not exist: {csv_file_class1}\")\n",
        "        return\n",
        "\n",
        "    # Process CSV files\n",
        "    csv_texts, csv_labels = process_csv_files(\n",
        "        csv_file_class0=csv_file_class0,\n",
        "        csv_file_class1=csv_file_class1,\n",
        "        text_column='Syllabus'  # Replace with your actual column name if different\n",
        "    )\n",
        "\n",
        "    # Print number of CSV texts and labels\n",
        "    print(f\"Number of CSV texts: {len(csv_texts)}\")\n",
        "    print(f\"Number of CSV labels: {len(csv_labels)}\")\n",
        "\n",
        "    # Combine labels for PDFs and CSVs\n",
        "    labels = labels_pdf.copy()\n",
        "    labels.extend(csv_labels)\n",
        "\n",
        "    # Print total number of texts and labels\n",
        "    total_texts = len(filenames) + len(csv_texts)\n",
        "    total_labels = len(labels)\n",
        "    print(f\"Total number of texts: {total_texts}\")\n",
        "    print(f\"Total number of labels: {total_labels}\")\n",
        "\n",
        "    # Call the test_models_and_decision function\n",
        "    decisions = test_models_and_decision(\n",
        "        folder_path=folder_path,\n",
        "        labels=labels,\n",
        "        add_text=None,        # Replace with your additional texts if any\n",
        "        csv_texts=csv_texts,\n",
        "        csv_labels=csv_labels,\n",
        "        x_percent=50\n",
        "    )\n",
        "\n",
        "    # Print the decisions\n",
        "    pdf_inputs = [os.path.join(folder_path, filename) for filename in filenames]\n",
        "    all_inputs = pdf_inputs.copy()\n",
        "    all_inputs.extend(csv_texts)  # Assuming add_text is None\n",
        "\n",
        "    print(\"\\nDecisions:\")\n",
        "    for input_item, decision in zip(all_inputs, decisions):\n",
        "        if isinstance(input_item, str) and input_item.lower().endswith('.pdf'):\n",
        "            display_text = os.path.basename(input_item)\n",
        "        else:\n",
        "            display_text = input_item[:100]  # Display first 100 characters\n",
        "        print(f\"Input: {display_text}...\")\n",
        "        print(f\"Decision: {decision}\")\n",
        "        print('---')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}